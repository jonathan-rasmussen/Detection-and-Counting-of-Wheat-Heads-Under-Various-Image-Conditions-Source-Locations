{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ECE657A_EfficientDet2.pynb","provenance":[],"authorship_tag":"ABX9TyOsolgq8Gn6LwyI18/ftILc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"39b5c6f35ce7450ba99cdc96c6bd7cef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_921a397539434e0180246ba223729ac7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_94dc4735333a4d76a3c67b4ff95c7c7a","IPY_MODEL_45da11776d3d41d699c6b467f4589b6e"]}},"921a397539434e0180246ba223729ac7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"94dc4735333a4d76a3c67b4ff95c7c7a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7e7d3efa5da44984acd8534b522fc665","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":3373,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3373,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e916dbdf301a46f296780847cb42d79f"}},"45da11776d3d41d699c6b467f4589b6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7aa13ba5ef7e4dd186e9d06ec83c00ed","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3373/3373 [11:23&lt;00:00,  4.94it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_820a8cd0891a4d5da3800f9a96cd17d3"}},"7e7d3efa5da44984acd8534b522fc665":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e916dbdf301a46f296780847cb42d79f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7aa13ba5ef7e4dd186e9d06ec83c00ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"820a8cd0891a4d5da3800f9a96cd17d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"AZWU08pAr6fv","colab_type":"text"},"source":["Using code from Signatrix: https://github.com/signatrix/efficientdet"]},{"cell_type":"code","metadata":{"id":"_oXCgWOXsG16","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"status":"ok","timestamp":1595360443535,"user_tz":240,"elapsed":7429,"user":{"displayName":"Corin Seelemann","photoUrl":"","userId":"02547318997313883062"}},"outputId":"cb356a58-c8c0-4d7b-d48f-2a03ba039195"},"source":["!git clone https://github.com/signatrix/efficientdet.git\n","%cd efficientdet/ \n","!pip install -r requirements.txt\n","\n","#import some useful modules\n","import pandas as pd\n","import numpy as np\n","from IPython.display import Image, clear_output  # to display images\n","#from utils.google_utils import gdrive_download  # to download models/datasets\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'efficientdet'...\n","remote: Enumerating objects: 144, done.\u001b[K\n","remote: Total 144 (delta 0), reused 0 (delta 0), pack-reused 144\u001b[K\n","Receiving objects: 100% (144/144), 28.03 MiB | 35.30 MiB/s, done.\n","Resolving deltas: 100% (68/68), done.\n","/content/efficientdet/efficientdet/efficientdet\n","Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.6.3)\n","Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2.1)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.0.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch->-r requirements.txt (line 1)) (1.5.1+cu101)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r requirements.txt (line 2)) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r requirements.txt (line 2)) (1.18.5)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r requirements.txt (line 2)) (3.12.2)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools->-r requirements.txt (line 3)) (0.29.21)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->-r requirements.txt (line 3)) (3.2.2)\n","Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->-r requirements.txt (line 3)) (49.1.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch->-r requirements.txt (line 1)) (0.16.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 3)) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 3)) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 3)) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 3)) (2.8.1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ep5EFuxIxJWU","colab_type":"text"},"source":["requirments.txt has the following modules: \n","efficientnet_pytorch\n","tensorboardX\n","pycocotools"]},{"cell_type":"code","metadata":{"id":"N1__Vzptxkoy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1596820630728,"user_tz":240,"elapsed":4671,"user":{"displayName":"Corin Seelemann","photoUrl":"","userId":"02547318997313883062"}},"outputId":"26db63bf-4458-4ba1-c0ca-dd84778bdf1e"},"source":["#mount at google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 729, in _input_request\n","    ident, reply = self.session.recv(self.stdin_socket, 0)\n","  File \"/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\", line 803, in recv\n","    msg_list = socket.recv_multipart(mode, copy=copy)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\", line 491, in recv_multipart\n","    parts = [self.recv(flags, copy=copy, track=track)]\n","  File \"zmq/backend/cython/socket.pyx\", line 791, in zmq.backend.cython.socket.Socket.recv\n","  File \"zmq/backend/cython/socket.pyx\", line 827, in zmq.backend.cython.socket.Socket.recv\n","  File \"zmq/backend/cython/socket.pyx\", line 186, in zmq.backend.cython.socket._recv_copy\n","  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-1-4dc5a40795b3>\", line 3, in <module>\n","    drive.mount('/content/drive')\n","  File \"/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\", line 242, in mount\n","    fifo_file.write(_getpass.getpass(auth_prompt) + '\\n')\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 687, in getpass\n","    password=True,\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 734, in _input_request\n","    raise KeyboardInterrupt\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.6/inspect.py\", line 742, in getmodule\n","    os.path.realpath(f)] = module.__name__\n","  File \"/usr/lib/python3.6/posixpath.py\", line 395, in realpath\n","    path, ok = _joinrealpath(filename[:0], filename, {})\n","  File \"/usr/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n","    if not islink(newpath):\n","  File \"/usr/lib/python3.6/posixpath.py\", line 171, in islink\n","    st = os.lstat(path)\n","KeyboardInterrupt\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2881\u001b[0m                 \u001b[0;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2882\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2883\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-4dc5a40795b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    241\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1822\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 1826\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1411\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1319\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m             )\n\u001b[1;32m   1321\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m                 \u001b[0mformatted_exceptions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_chained_exception_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: must be str, not list"]}]},{"cell_type":"code","metadata":{"id":"qsFuVK_gyQ2j","colab_type":"code","colab":{}},"source":["#create dataframe 'df' to reformat training images for effifcientdet\n","import tqdm\n","df = pd.read_csv('/content/drive/My Drive/Copy of wheat/train.csv')\n","bboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\n","for i, column in enumerate(['x', 'y', 'w', 'h']):\n","    df[column] = bboxs[:,i]\n","df.drop(columns=['bbox'], inplace=True)\n","df['x_center'] = df['x'] + df['w']/2\n","df['y_center'] = df['y'] + df['h']/2\n","df['classes'] = 0\n","from tqdm.auto import tqdm\n","import shutil as sh\n","df = df[['image_id','x', 'y', 'w', 'h','x_center','y_center','classes']]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MEz6gS70ysUS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595360165087,"user_tz":240,"elapsed":1405493,"user":{"displayName":"Corin Seelemann","photoUrl":"","userId":"02547318997313883062"}},"outputId":"eb742e73-f21f-438d-fa05-2ddf868f8957"},"source":["!git clone https://github.com/NVIDIA/apex && cd apex && pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" . --user && cd .. && rm -rf apex"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'apex'...\n","remote: Enumerating objects: 14, done.\u001b[K\n","remote: Counting objects:   7% (1/14)\u001b[K\rremote: Counting objects:  14% (2/14)\u001b[K\rremote: Counting objects:  21% (3/14)\u001b[K\rremote: Counting objects:  28% (4/14)\u001b[K\rremote: Counting objects:  35% (5/14)\u001b[K\rremote: Counting objects:  42% (6/14)\u001b[K\rremote: Counting objects:  50% (7/14)\u001b[K\rremote: Counting objects:  57% (8/14)\u001b[K\rremote: Counting objects:  64% (9/14)\u001b[K\rremote: Counting objects:  71% (10/14)\u001b[K\rremote: Counting objects:  78% (11/14)\u001b[K\rremote: Counting objects:  85% (12/14)\u001b[K\rremote: Counting objects:  92% (13/14)\u001b[K\rremote: Counting objects: 100% (14/14)\u001b[K\rremote: Counting objects: 100% (14/14), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 7367 (delta 3), reused 0 (delta 0), pack-reused 7353\u001b[K\n","Receiving objects: 100% (7367/7367), 13.89 MiB | 23.74 MiB/s, done.\n","Resolving deltas: 100% (4974/4974), done.\n","/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n","  cmdoptions.check_install_build_global(options)\n","Created temporary directory: /tmp/pip-ephem-wheel-cache-puj8l9_8\n","Created temporary directory: /tmp/pip-req-tracker-66i0nhrh\n","Created requirements tracker '/tmp/pip-req-tracker-66i0nhrh'\n","Created temporary directory: /tmp/pip-install-lm6h1nlx\n","Processing /content/efficientdet/apex\n","  Created temporary directory: /tmp/pip-req-build-r6ol_8qf\n","  Added file:///content/efficientdet/apex to build tracker '/tmp/pip-req-tracker-66i0nhrh'\n","    Running setup.py (path:/tmp/pip-req-build-r6ol_8qf/setup.py) egg_info for package from file:///content/efficientdet/apex\n","    Running command python setup.py egg_info\n","\n","    Warning: Torch did not find available GPUs on this system.\n","     If your intention is to cross-compile, this is not an error.\n","    By default, Apex will cross-compile for Pascal (compute capabilities 6.0, 6.1, 6.2),\n","    Volta (compute capability 7.0), and Turing (compute capability 7.5).\n","    If you wish to cross-compile for a single specific architecture,\n","    export TORCH_CUDA_ARCH_LIST=\"compute capability\" before running setup.py.\n","\n","\n","\n","    torch.__version__  = 1.5.1+cu101\n","\n","\n","    running egg_info\n","    creating /tmp/pip-req-build-r6ol_8qf/pip-egg-info/apex.egg-info\n","    writing /tmp/pip-req-build-r6ol_8qf/pip-egg-info/apex.egg-info/PKG-INFO\n","    writing dependency_links to /tmp/pip-req-build-r6ol_8qf/pip-egg-info/apex.egg-info/dependency_links.txt\n","    writing top-level names to /tmp/pip-req-build-r6ol_8qf/pip-egg-info/apex.egg-info/top_level.txt\n","    writing manifest file '/tmp/pip-req-build-r6ol_8qf/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    writing manifest file '/tmp/pip-req-build-r6ol_8qf/pip-egg-info/apex.egg-info/SOURCES.txt'\n","    /tmp/pip-req-build-r6ol_8qf/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","  Source in /tmp/pip-req-build-r6ol_8qf has version 0.1, which satisfies requirement apex==0.1 from file:///content/efficientdet/apex\n","  Removed apex==0.1 from file:///content/efficientdet/apex from build tracker '/tmp/pip-req-tracker-66i0nhrh'\n","Skipping wheel build for apex, due to binaries being disabled for it.\n","Installing collected packages: apex\n","  Created temporary directory: /tmp/pip-record-kao_g18o\n","    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-r6ol_8qf/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-r6ol_8qf/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-kao_g18o/install-record.txt --single-version-externally-managed --compile --user --prefix=\n","\n","    Warning: Torch did not find available GPUs on this system.\n","     If your intention is to cross-compile, this is not an error.\n","    By default, Apex will cross-compile for Pascal (compute capabilities 6.0, 6.1, 6.2),\n","    Volta (compute capability 7.0), and Turing (compute capability 7.5).\n","    If you wish to cross-compile for a single specific architecture,\n","    export TORCH_CUDA_ARCH_LIST=\"compute capability\" before running setup.py.\n","\n","\n","\n","    torch.__version__  = 1.5.1+cu101\n","\n","\n","    /tmp/pip-req-build-r6ol_8qf/setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n","      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n","    No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n","\n","    Compiling cuda extensions with\n","    nvcc: NVIDIA (R) Cuda compiler driver\n","    Copyright (c) 2005-2019 NVIDIA Corporation\n","    Built on Sun_Jul_28_19:07:16_PDT_2019\n","    Cuda compilation tools, release 10.1, V10.1.243\n","    from /usr/local/cuda/bin\n","\n","    running install\n","    running build\n","    running build_py\n","    creating build\n","    creating build/lib.linux-x86_64-3.6\n","    creating build/lib.linux-x86_64-3.6/apex\n","    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n","    creating build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n","    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n","    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n","    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n","    creating build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof\n","    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n","    creating build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n","    creating build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n","    creating build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n","    creating build/lib.linux-x86_64-3.6/apex/contrib\n","    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n","    creating build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n","    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n","    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n","    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n","    running build_ext\n","    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:305: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n","      warnings.warn(msg.format('we could not find ninja.'))\n","    building 'apex_C' extension\n","    creating build/temp.linux-x86_64-3.6\n","    creating build/temp.linux-x86_64-3.6/csrc\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from csrc/flatten_unflatten.cpp:2:0:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function â€˜at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()â€™:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         return tensors[0].type();\n","                                ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/flatten_unflatten.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'amp_C' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_62,code=sm_62 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_62,code=sm_62 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_62,code=sm_62 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_62,code=sm_62 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_62,code=sm_62 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_62,code=sm_62 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_62,code=sm_62 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_62,code=sm_62 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_62,code=sm_62 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_62,code=sm_62 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n","    building 'syncbn' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_62,code=sm_62 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n","    building 'fused_layer_norm_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function â€˜std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)â€™:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function â€˜std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)â€™:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function â€˜at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)â€™:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp: In function â€˜std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)â€™:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(dout);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(mean);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(invvar);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(input);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(gamma);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    csrc/layer_norm_cuda.cpp:117:42: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                                              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:141:65: note: in definition of macro â€˜C10_UNLIKELYâ€™\n","     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n","                                                                     ^~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:262:7: note: in expansion of macro â€˜C10_UNLIKELY_OR_CONSTâ€™\n","       if (C10_UNLIKELY_OR_CONST(!(cond))) {                     \\\n","           ^~~~~~~~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:273:32: note: in expansion of macro â€˜TORCH_CHECK_WITHâ€™\n","     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n","                                    ^~~~~~~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro â€˜TORCH_CHECKâ€™\n","     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n","                           ^~~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro â€˜CHECK_CUDAâ€™\n","     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n","                            ^~~~~~~~~~\n","    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro â€˜CHECK_INPUTâ€™\n","       CHECK_INPUT(beta);\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/layer_norm_cuda.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_62,code=sm_62 -std=c++14\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n","    building 'mlp_cuda' extension\n","    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n","    csrc/mlp.cpp: In function â€˜std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)â€™:\n","    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:64:77: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n","                                                                                 ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:67: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of â€˜reserved_sizeâ€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n","                                                                        ^\n","    csrc/mlp.cpp:65:68: warning: narrowing conversion of â€˜reserved_sizeâ€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:67:54: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: â€˜c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)â€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp:76:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = mlp_fp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:67:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In function â€˜std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)â€™:\n","    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < num_layers; i++) {\n","                       ~~^~~~~~~~~~~~\n","    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","       for (int i = 0; i < inputs.size(); i++) {\n","                       ~~^~~~~~~~~~~~~~~\n","    csrc/mlp.cpp:120:67: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n","                                                                       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:123:54: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","                                                          ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:129:28: note: in definition of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","         const auto& the_type = TYPE;                                             \\\n","                                ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:131:56: warning: â€˜c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)â€™ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n","         at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n","                                                            ^\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:46:23: note: declared here\n","     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n","                           ^~~~~~~~~~~\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp: In lambda function:\n","    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < num_layers; i++) {\n","                         ~~^~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n","         for (int i = 0; i < inputs.size(); i++) {\n","                         ~~^~~~~~~~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:80: warning: â€˜at::DeprecatedTypeProperties& at::Tensor::type() constâ€™ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                                                                    ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:11:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:5,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:262:30: note: declared here\n","       DeprecatedTypeProperties & type() const {\n","                                  ^~~~\n","    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9:0,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:4,\n","                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n","                     from csrc/mlp.cpp:1:\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:137:44: warning: narrowing conversion of â€˜(work_size / sizeof (scalar_t))â€™ from â€˜long unsigned intâ€™ to â€˜long intâ€™ inside { } [-Wnarrowing]\n","         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n","                                      ~~~~~~~~~~^~~~~~~~~\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    csrc/mlp.cpp:139:10: warning: unused variable â€˜resultâ€™ [-Wunused-variable]\n","         auto result = mlp_bp<scalar_t>(\n","              ^\n","    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:12:12: note: in definition of macro â€˜AT_PRIVATE_CASE_TYPEâ€™\n","         return __VA_ARGS__();                          \\\n","                ^~~~~~~~~~~\n","    csrc/mlp.cpp:123:3: note: in expansion of macro â€˜AT_DISPATCH_FLOATING_TYPES_AND_HALFâ€™\n","       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n","       ^\n","    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_62,code=sm_62 -std=c++14\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(100): warning: attribute \"__visibility__\" does not apply here\n","\n","    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/autograd/profiler.h(115): warning: attribute \"__visibility__\" does not apply here\n","\n","    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n","    running install_lib\n","    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n","    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n","    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n","    creating /root/.local/lib/python3.6/site-packages/apex\n","    creating /root/.local/lib/python3.6/site-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    creating /root/.local/lib/python3.6/site-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /root/.local/lib/python3.6/site-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /root/.local/lib/python3.6/site-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /root/.local/lib/python3.6/site-packages/apex/amp/lists\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /root/.local/lib/python3.6/site-packages/apex/amp\n","    creating /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n","    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /root/.local/lib/python3.6/site-packages/apex/fp16_utils\n","    creating /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply\n","    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply\n","    creating /root/.local/lib/python3.6/site-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /root/.local/lib/python3.6/site-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /root/.local/lib/python3.6/site-packages/apex/reparameterization\n","    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/reparameterization\n","    creating /root/.local/lib/python3.6/site-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /root/.local/lib/python3.6/site-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/mlp\n","    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /root/.local/lib/python3.6/site-packages/apex\n","    creating /root/.local/lib/python3.6/site-packages/apex/pyprof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof\n","    creating /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/prof\n","    creating /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx\n","    creating /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n","    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /root/.local/lib/python3.6/site-packages/apex/pyprof/parse\n","    creating /root/.local/lib/python3.6/site-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/normalization\n","    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /root/.local/lib/python3.6/site-packages/apex/normalization\n","    creating /root/.local/lib/python3.6/site-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /root/.local/lib/python3.6/site-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /root/.local/lib/python3.6/site-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/RNN\n","    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /root/.local/lib/python3.6/site-packages/apex/RNN\n","    creating /root/.local/lib/python3.6/site-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n","    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /root/.local/lib/python3.6/site-packages/apex/parallel\n","    creating /root/.local/lib/python3.6/site-packages/apex/contrib\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib\n","    creating /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity\n","    creating /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn\n","    creating /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn\n","    creating /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy\n","    creating /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers\n","    creating /root/.local/lib/python3.6/site-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /root/.local/lib/python3.6/site-packages/apex/optimizers\n","    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n","    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /root/.local/lib/python3.6/site-packages\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/utils.py to utils.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/amp.py to amp.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/compat.py to compat.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/opt.py to opt.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/handle.py to handle.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/RNN/models.py to models.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n","    byte-compiling /root/.local/lib/python3.6/site-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n","    running install_egg_info\n","    running egg_info\n","    creating apex.egg-info\n","    writing apex.egg-info/PKG-INFO\n","    writing dependency_links to apex.egg-info/dependency_links.txt\n","    writing top-level names to apex.egg-info/top_level.txt\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    writing manifest file 'apex.egg-info/SOURCES.txt'\n","    Copying apex.egg-info to /root/.local/lib/python3.6/site-packages/apex-0.1-py3.6.egg-info\n","    running install_scripts\n","    writing list of installed files to '/tmp/pip-record-kao_g18o/install-record.txt'\n","    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n","  Removing source in /tmp/pip-req-build-r6ol_8qf\n","Successfully installed apex-0.1\n","Cleaning up...\n","Removed build tracker '/tmp/pip-req-tracker-66i0nhrh'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bwPrlTxa4LF8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["39b5c6f35ce7450ba99cdc96c6bd7cef","921a397539434e0180246ba223729ac7","94dc4735333a4d76a3c67b4ff95c7c7a","45da11776d3d41d699c6b467f4589b6e","7e7d3efa5da44984acd8534b522fc665","e916dbdf301a46f296780847cb42d79f","7aa13ba5ef7e4dd186e9d06ec83c00ed","820a8cd0891a4d5da3800f9a96cd17d3"]},"executionInfo":{"status":"ok","timestamp":1595361212532,"user_tz":240,"elapsed":684226,"user":{"displayName":"Corin Seelemann","photoUrl":"","userId":"02547318997313883062"}},"outputId":"bc7dc8bb-acb0-4ff8-a141-ea08506bf9ee"},"source":["import os \n","index = list(set(df.image_id))\n","source = 'train'\n","if True:\n","    for fold in [0]:\n","        val_index = index[len(index)*fold//5:len(index)*(fold+1)//5]\n","        for name,mini in tqdm(df.groupby('image_id')):\n","            if name in val_index:\n","                path2save = 'val2017/'\n","            else:\n","                path2save = 'train2017/'\n","            if not os.path.exists('/content/wheat1/fold{}/labels/'.format(fold)+path2save):\n","                os.makedirs('/content/wheat1/fold{}/labels/'.format(fold)+path2save)\n","            with open('/content/wheat1/fold{}/labels/'.format(fold)+path2save+name+\".txt\", 'w+') as f:\n","                row = mini[['classes','x_center','y_center','w','h']].astype(float).values\n","                row = row/1024\n","                row = row.astype(str) \n","                for j in range(len(row)):\n","                    text = ' '.join(row[j])\n","                    f.write(text)\n","                    f.write(\"\\n\")\n","            if not os.path.exists('/content/wheat1/fold{}/images/{}'.format(fold,path2save)):\n","                os.makedirs('/content/wheat1/fold{}/images/{}'.format(fold,path2save))\n","            sh.copy(\"/content/drive/My Drive/Copy of wheat/{}/{}.jpg\".format(source,name),'/content/wheat1/fold{}/images/{}/{}.jpg'.format(fold,path2save,name))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39b5c6f35ce7450ba99cdc96c6bd7cef","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=3373.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7fGL5_k59kEJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1595361952983,"user_tz":240,"elapsed":3021,"user":{"displayName":"Corin Seelemann","photoUrl":"","userId":"02547318997313883062"}},"outputId":"34b94e3f-94b8-4a49-fd13-90d0836206a9"},"source":["#run train.py \n","!python train.py --image_size 1024 --batch_size 2 --num_epochs 50 --data_path '/content/wheat1/'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n","Traceback (most recent call last):\n","  File \"train.py\", line 189, in <module>\n","    train(opt)\n","  File \"train.py\", line 58, in train\n","    transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n","  File \"/content/efficientdet/efficientdet/efficientdet/src/dataset.py\", line 17, in __init__\n","    self.coco = COCO(os.path.join(self.root_dir, 'annotations', 'instances_' + self.set_name + '.json'))\n","  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/coco.py\", line 84, in __init__\n","    dataset = json.load(open(annotation_file, 'r'))\n","FileNotFoundError: [Errno 2] No such file or directory: '/content/wheat1/annotations/instances_train2017.json'\n"],"name":"stdout"}]}]}